{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP6 NN (OBLIGATORY)\n",
    " In this TP we will develop a two-layer fully-connected neural network to perform classification, and test it out on the CIFAR-10 dataset. We train the network with a softmax loss function. The network uses a ReLU nonlinearity activation function after the first fully connected layer. The outputs of the second fully-connected layer are the scores for each class.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_IRIS, load_CIFAR10\n",
    "from NN import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# make figures appear inline\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The CIFAR10 dataset contains 60,000 32x32 color images in 10 different classes ([CIFAR-10](https://en.wikipedia.org/wiki/CIFAR-10)) \n",
    "\n",
    "You have to download the dataset; open a terminal and go to the folder *datasets*, then execute the script *get_datasets.sh*:\n",
    "```bash\n",
    "$ ./get_datasets.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "We will use the class NN in the file NN.py to represent instances of our network. The network parameters are stored in the instance variable self.params where keys are string parameter names and values are numpy arrays. \n",
    "\n",
    "- Open the file NN.py and fill the missing parts of the NN.loss. This function takes the data and weights and computes the class scores, the loss, and the gradients on the parameters. (You have to perform the forward pass, the backward pass).\n",
    "\n",
    "- Fill the missing part in the NN.train() (You have just to to update the parameters of the network). \n",
    "\n",
    "- Implement the NN.predict method to predict labels for data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Train your network using 1, 10, 100, 1000 neurons in the hidden layer (hidden_size) and for each case compute the test accuracy and calculate the time it takes to train the network. Explain if and how the size of the hidden layer influences the prediction (test accuracy) and the time it takes to train the network.\n",
    "\n",
    "#### To train your network use: learning_rate=1e-4, learning_rate_decay=0.95, batch_size=100, num_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = np.array([1, 10, 100, 1000])\n",
    "num_classes = 10\n",
    "\n",
    "for size in hidden_size:\n",
    "    start = timer()\n",
    "    Neural_Network = NN(input_size=input_size, hidden_size=size, output_size=num_classes)\n",
    "    loss_history, train_acc_history, test_acc_history = Neural_Network.train(X=X_train, y=y_train, X_test=X_test, y_test=y_test,\n",
    "                  learning_rate=1e-4, learning_rate_decay=0.95, num_iters=1000,\n",
    "                  batch_size=100, verbose=False)\n",
    "    end = timer()\n",
    "    print('hidden layer size: ', size)\n",
    "    print('seconds elapsed: ', end - start)\n",
    "    print('train_acc_history: ', train_acc_history)\n",
    "    print('test_acc_history: ', test_acc_history)\n",
    "    #### Plot the train / test accuracies\n",
    "    with PdfPages(\"fig{0}.pdf\".format(size)) as pdf:\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(loss_history)\n",
    "        plt.title('Loss history {0} neurons'.format(size))\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(train_acc_history, label='train')\n",
    "        plt.plot(test_acc_history, label='test')\n",
    "        plt.title('Classification accuracy history')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Clasification accuracy')\n",
    "        plt.legend()\n",
    "        pdf.savefig()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON WITH SAME NETWORK ARCHITECTURE USING KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train).astype(float)\n",
    "y_train_df = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names first\n",
    "names = X_train_df.columns\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(X_train_df)\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranschubert/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/kieranschubert/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train_df)\n",
    "encoded_Y = encoder.transform(y_train_df)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(10, input_dim=3072, activation='relu'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=3, batch_size=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kieranschubert/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/kieranschubert/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Baseline: 19.07% (2.82%)\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X_train_df, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same network using the Keras library gives a CV correct classification of 19.07%, which is lower than the test correct classification of around 33% using our own Neural Network."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
