{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 Softmax - Gradient descent - L2 norm regularizer\n",
    "## OBLIGATORY\n",
    "### Deadline 29/04/2019 23:59\n",
    "\n",
    "\n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- implement the **loss function** for the Softmax classifier\n",
    "- implement the expression for its **analytic gradient**\n",
    "- implement L2 norm regularizer\n",
    "- **optimize** the loss function with **SGD**\n",
    "- use online, full batch and mini batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_IRIS, load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (100, 4)\n",
      "Training labels shape:  (100,)\n",
      "Test data shape:  (50, 4)\n",
      "Test labels shape:  (50,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_IRIS(test=True)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "### 1 Fill the missing part of the SoftamaxClassifier() class.\n",
    "Your code  will  be written inside **Softmax_classifier.py**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fill the missing part in the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieranschubert/Desktop/Statistics/2nd_Semester/Data_Mining/2019/TP4_softmax_GD_L2regularizer-20190416/Softmax_classifier.py:77: RuntimeWarning: invalid value encountered in true_divide\n",
      "  calc_loss = (-1/X.shape[0])*np.sum(indicator*np.log(nom/denom), axis=1).sum()\n",
      "/Users/kieranschubert/Desktop/Statistics/2nd_Semester/Data_Mining/2019/TP4_softmax_GD_L2regularizer-20190416/Softmax_classifier.py:77: RuntimeWarning: divide by zero encountered in log\n",
      "  calc_loss = (-1/X.shape[0])*np.sum(indicator*np.log(nom/denom), axis=1).sum()\n",
      "/Users/kieranschubert/Desktop/Statistics/2nd_Semester/Data_Mining/2019/TP4_softmax_GD_L2regularizer-20190416/Softmax_classifier.py:77: RuntimeWarning: invalid value encountered in multiply\n",
      "  calc_loss = (-1/X.shape[0])*np.sum(indicator*np.log(nom/denom), axis=1).sum()\n",
      "/Users/kieranschubert/Desktop/Statistics/2nd_Semester/Data_Mining/2019/TP4_softmax_GD_L2regularizer-20190416/Softmax_classifier.py:113: RuntimeWarning: invalid value encountered in true_divide\n",
      "  grad[i] = const*np.sum(np.repeat(X[:,i], num_classes).reshape((X.shape[0], num_classes))*(indicator - nom/denom), axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn_accuracy_online:  [[0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41]\n",
      " [0.41 0.41 0.41 0.46 0.57 0.7  0.95 0.41]\n",
      " [0.61 0.73 0.73 0.86 0.73 0.59 0.41 0.41]\n",
      " [0.73 0.61 0.75 0.91 0.97 0.73 0.32 0.41]\n",
      " [0.91 0.95 0.96 0.64 0.96 0.27 0.41 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]]\n",
      "best_trn_accuracy_online:  0.97\n",
      "test_accuracy_online:  0.82\n",
      "opt_lr: 0.1 opt_reg: 0.1\n",
      "trn_accuracy_fullBatch:  [[0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41]\n",
      " [0.41 0.41 0.41 0.45 0.62 0.71 0.7  0.41]\n",
      " [0.73 0.73 0.74 0.74 0.82 0.76 0.41 0.41]\n",
      " [0.91 0.95 0.96 0.97 0.96 0.74 0.41 0.41]\n",
      " [0.95 0.97 0.97 0.59 0.95 0.59 0.27 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]]\n",
      "best_trn_accuracy_fullBatch:  0.97\n",
      "test_accuracy_fullBatch:  0.74\n",
      "opt_lr: 0.01 opt_reg: 0.1\n",
      "trn_accuracy_miniBatch:  [[0.41 0.41 0.41 0.41 0.41 0.41 0.41 0.41]\n",
      " [0.41 0.41 0.41 0.45 0.57 0.71 0.69 0.41]\n",
      " [0.73 0.73 0.74 0.74 0.81 0.74 0.41 0.41]\n",
      " [0.9  0.98 0.94 0.92 0.97 0.82 0.41 0.27]\n",
      " [0.97 0.95 0.92 0.59 0.97 0.73 0.41 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]\n",
      " [0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.32]]\n",
      "best_trn_accuracy_miniBatch:  0.98\n",
      "test_accuracy_miniBatch:  0.18\n",
      "opt_lr: 0.0001 opt_reg: 0.1\n",
      "best training accuracy achieved : 0.980000\n",
      "best test accuracy achieved : 0.820000\n"
     ]
    }
   ],
   "source": [
    "# Run the softhmax classifier for dirrerent hyperparameters (regularization strength and\n",
    "# learning rate). \n",
    "# Set the number of the iteration equal to 500 iteration and add a stopping criterion.\n",
    "from collections import defaultdict\n",
    "from Softmax_classifier import SoftmaxClassifier\n",
    "softmax_classifier = SoftmaxClassifier()\n",
    "results = {}\n",
    "best_train = -1\n",
    "best_train_softmax = None\n",
    "best_test = -1\n",
    "best_test_softmax = None\n",
    "learning_rates = [0.00001, 0.0001, 0.001, 0.01, 0.1, 0, 1, 10]\n",
    "regularization_strengths = [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100]\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Train the Softmax classifier for 500 iterations (and add a stopping criterion)#\n",
    "#for different learning rate and \n",
    "# regularization strength using online, full batch and mini batch GD                                                                             #\n",
    "#                                                                              #\n",
    "# compute the train and test accuracy for each case.                           #                                                     #\n",
    "# save the best train softmax classifer in best_train_softmax.                 #\n",
    "# and the best test softmax classifer in best_test_softmax.                    #\n",
    "#                                                                              #\n",
    "#                                                                              #\n",
    "#                                                                              #\n",
    "################################################################################\n",
    "# Your code\n",
    "\n",
    "best_acc_train = np.zeros(3)\n",
    "best_acc_test = np.zeros(3)\n",
    "\n",
    "#ONLINE GD TRAINING\n",
    "train_accuracy_online = np.zeros((len(learning_rates), len(regularization_strengths)))\n",
    "for j in range(0, len(learning_rates)):\n",
    "    for i in range(0, len(regularization_strengths)):\n",
    "        loss_online_train, w_opt_online = softmax_classifier.train(X_train, y_train, learning_rate=learning_rates[j], reg=regularization_strengths[i], num_iters=500, batch_size=1)\n",
    "        y_pred_online_train = softmax_classifier.predict(X_train, y_train, w_opt_online)\n",
    "        train_accuracy_online[j, i] = ((y_pred_online_train - y_train) == 0).sum() / len(y_pred_online_train)\n",
    "        \n",
    "print(\"trn_accuracy_online: \",train_accuracy_online)  \n",
    "opt_reg_online, opt_lr_online = np.unravel_index(np.argmax(train_accuracy_online, axis=None), train_accuracy_online.shape)\n",
    "print(\"best_trn_accuracy_online: \",train_accuracy_online[opt_reg_online, opt_lr_online]) \n",
    "\n",
    "#ONLINE GD TEST\n",
    "#Uses the parameters learned from the training set\n",
    "from Softmax_classifier import SoftmaxClassifier\n",
    "softmax_classifier = SoftmaxClassifier()\n",
    "\n",
    "loss_online_test, w_opt_online_test = softmax_classifier.train(X_train, y_train, learning_rate=learning_rates[opt_lr_online], reg=regularization_strengths[opt_reg_online], num_iters=500, batch_size=1)\n",
    "y_pred_online_test = softmax_classifier.predict(X_test, y_test, w_opt_online_test)\n",
    "test_accuracy_online = ((y_pred_online_test - y_test) == 0).sum() / len(y_pred_online_test)\n",
    "print(\"test_accuracy_online: \", test_accuracy_online)\n",
    "print(\"opt_lr:\", learning_rates[opt_lr_online], \"opt_reg:\", regularization_strengths[opt_reg_online])\n",
    "\n",
    "best_acc_train[0] =  train_accuracy_online[opt_reg_online,opt_lr_online]\n",
    "best_acc_test[0] = test_accuracy_online\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FULL BATCH GD TRAINING\n",
    "from Softmax_classifier import SoftmaxClassifier\n",
    "softmax_classifier = SoftmaxClassifier()\n",
    "\n",
    "train_accuracy_fullBatch = np.zeros((len(learning_rates), len(regularization_strengths)))\n",
    "for j in range(0, len(learning_rates)):\n",
    "    for i in range(0, len(regularization_strengths)):\n",
    "        loss_full_train, w_opt_full = softmax_classifier.train(X_train, y_train, learning_rate=learning_rates[j], reg=regularization_strengths[i], num_iters=500, batch_size=X_train.shape[0])\n",
    "        y_pred_full_train = softmax_classifier.predict(X_train, y_train, w_opt_full)\n",
    "        train_accuracy_fullBatch[j, i] = ((y_pred_full_train - y_train) == 0).sum() / len(y_pred_full_train)\n",
    "        \n",
    "print(\"trn_accuracy_fullBatch: \",train_accuracy_fullBatch)  \n",
    "opt_reg_full, opt_lr_full = np.unravel_index(np.argmax(train_accuracy_fullBatch, axis=None), train_accuracy_fullBatch.shape)\n",
    "print(\"best_trn_accuracy_fullBatch: \",train_accuracy_fullBatch[opt_reg_full, opt_lr_full]) \n",
    "\n",
    "#FULL BATCH GD TEST \n",
    "#Uses the parameters learned from the training set\n",
    "from Softmax_classifier import SoftmaxClassifier\n",
    "softmax_classifier = SoftmaxClassifier()\n",
    "\n",
    "loss_full_test, w_opt_full_test = softmax_classifier.train(X_train, y_train, learning_rate=learning_rates[opt_lr_full], reg=regularization_strengths[opt_reg_full], num_iters=500, batch_size=X_train.shape[0])\n",
    "y_pred_full_test = softmax_classifier.predict(X_test, y_test, w_opt_full_test)\n",
    "test_accuracy_fullBatch = ((y_pred_full_test - y_test) == 0).sum() / len(y_pred_full_test)\n",
    "print(\"test_accuracy_fullBatch: \", test_accuracy_fullBatch)\n",
    "print(\"opt_lr:\", learning_rates[opt_lr_full], \"opt_reg:\", regularization_strengths[opt_reg_full])\n",
    "\n",
    "best_acc_train[1] =  train_accuracy_fullBatch[opt_reg_full, opt_lr_full]\n",
    "best_acc_test[1] = test_accuracy_fullBatch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MINI-BATCH GD TRAINING\n",
    "from Softmax_classifier import SoftmaxClassifier\n",
    "softmax_classifier = SoftmaxClassifier()\n",
    "\n",
    "train_accuracy_miniBatch = np.zeros((len(learning_rates), len(regularization_strengths)))\n",
    "for j in range(0, len(learning_rates)):\n",
    "    for i in range(0, len(regularization_strengths)):\n",
    "        loss_mini_train, w_opt_mini_train = softmax_classifier.train(X_train, y_train, learning_rate=learning_rates[j], reg=regularization_strengths[i], num_iters=500, batch_size=20)\n",
    "        y_pred_mini_train = softmax_classifier.predict(X_train, y_train, w_opt_mini_train)\n",
    "        train_accuracy_miniBatch[j, i] = ((y_pred_mini_train - y_train) == 0).sum() / len(y_pred_mini_train)\n",
    "    \n",
    "print(\"trn_accuracy_miniBatch: \",train_accuracy_miniBatch)  \n",
    "opt_reg_mini, opt_lr_mini = np.unravel_index(np.argmax(train_accuracy_miniBatch, axis=None), train_accuracy_miniBatch.shape)\n",
    "print(\"best_trn_accuracy_miniBatch: \",train_accuracy_miniBatch[opt_reg_mini, opt_lr_mini]) \n",
    "\n",
    "\n",
    "#MINI-BATCH GD TEST\n",
    "#Uses the parameters learned from the training set\n",
    "from Softmax_classifier import SoftmaxClassifier\n",
    "softmax_classifier = SoftmaxClassifier()\n",
    "\n",
    "loss_mini_test, w_opt_mini_test = softmax_classifier.train(X_train, y_train, learning_rate=learning_rates[opt_lr_mini], reg=regularization_strengths[opt_reg_mini], num_iters=500, batch_size=20)\n",
    "y_pred_mini_test = softmax_classifier.predict(X_test, y_test, w_opt_mini_test)\n",
    "test_accuracy_miniBatch = ((y_pred_mini_test - y_test) == 0).sum() / len(y_pred_mini_test)\n",
    "print(\"test_accuracy_miniBatch: \", test_accuracy_miniBatch)\n",
    "print(\"opt_lr:\", learning_rates[opt_lr_mini], \"opt_reg:\", regularization_strengths[opt_reg_mini])\n",
    "\n",
    "best_acc_train[2] =  train_accuracy_miniBatch[opt_reg_mini, opt_lr_mini]\n",
    "best_acc_test[2] = test_accuracy_miniBatch\n",
    "\n",
    "\n",
    "best_train = best_acc_train[np.argmax(best_acc_train)]\n",
    "best_test = best_acc_test[np.argmax(best_acc_test)]\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "###############################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for batch_size, lr, reg in sorted(results):\n",
    "    train_accuracy, test_accuracy = results[(batch_size, lr, reg)]\n",
    "    print('batch_size %e lr %e reg %e train accuracy: %f test accuracy: %f' % (\n",
    "                batch_size, lr, reg, train_accuracy, test_accuracy))\n",
    "    \n",
    "print('best training accuracy achieved : %f' % best_train)\n",
    "print('best test accuracy achieved : %f' % best_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n",
      "[0.97 0.97 0.98]\n",
      "[0.82 0.74 0.18]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(best_acc_train))\n",
    "print(np.argmax(best_acc_test))\n",
    "print(best_acc_train)\n",
    "print(best_acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
